# -*- coding: utf-8 -*-
"""Search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFwf6HVUyog5CJI41OI1eOUrER5iFRTK
"""

#install Libraries
# !pip3 install pystemmer

#import Files
import nltk
from nltk.corpus import stopwords 
import bz2
from nltk.stem.porter import *
import sys
import timeit
nltk.download('stopwords',quiet=True)
import Stemmer
from collections import defaultdict
from bisect import bisect
from math import log10
from operator import itemgetter
import re

#global variables 
indexPath="/content/drive/My Drive/InvertedIndexFiles" #Path to invertedIndexFiles
DocIdtoTitle={} 
totalDocs=0 #total Documents in the entire Dump
secondaryIndex=[] #secondayIndex file values
queries=[] #search queries
queryPath="/content/queryFile.txt"
searchResult={} #stores result of each query

#fetch inital required Data from files

#read titles
def getTitle():
  global DocIdtoTitle,indexPath,totalDocs
  pathTitleFile="/content/drive/My Drive/Titles/id_title.txt"
  with open(pathTitleFile, 'r') as infile:
    fileLines=infile.readlines()
    for line in fileLines:
      ID, title = line.split(':',1)
      DocIdtoTitle[ID]=title
      totalDocs=len(DocIdtoTitle)
  infile.close()
  print("totalDocs: ",totalDocs)
  getSecondary()

# read secondary index
def getSecondary():
  global DocIdtoTitle,indexPath,totalDocs,secondaryIndex
  filePath=indexPath+"/secondaryIndex.txt"
  with open(filePath, 'r') as infile:
    fileLines=infile.readlines()
    for line in fileLines:
      firstWord=line.split(":")[0]
      secondaryIndex.append(firstWord)
  infile.close()

#read query file 
def readQueries():
  global queries,queryPath
  with open(queryPath, 'r') as file:
    queries = file.readlines()
  file.close()

#Process search Queries : remove stopwords,stemmize,regex
def processQuery(query):
  #lowerCase
  query=query.lower()
  
  #remove space unnecessary characters
  re6 = re.compile(r'[\_]', re.DOTALL)
  query = re6.sub(' ', query)
  query = re.findall("\d+|[\w]+", str(query))
  
  #remove stopwords
  stop_word = set(stopwords.words('english'))
  query = [w for w in query if w not in stop_word]

  #stemmize
  stemmer=Stemmer.Stemmer('english')
  stemmed_data=[]
  for words in query:
    stemmed_data.append(stemmer.stemWord(words))
  query=stemmed_data
  return query

#Get Posting list from the invertedIndex file and split by '|' 
def processPostingList(postingList):
  removeWord=postingList.split(":",1)
  docList=removeWord[1].split("|")
  return docList

def getList(fileNo,word):
  global indexPath
  fileName=indexPath+"/" + str(fileNo) + '.txt'
  filePtr = open(fileName, 'r')
  fileData=filePtr.read()
  finder=re.search('^'+word+':', fileData, re.M)
  if (finder):
     start_index = finder.start()
     next_index=start_index + 1
     end_index = fileData.find('\n', next_index)
     postingList=fileData[start_index:end_index]
    #  print("postingList: ",postingList)
     return processPostingList(postingList)

#Plain Query Functions

def calculateWeight(tagValue):
  tag=tagValue[0]
  freq=int(tagValue[1:])
  # print("freq:",freq)
  score={'t':100000,'b':1,'i':1,'l':1,'r':1,'c':1}
  return freq*score[tag]

def calculateScore(word):
  global DocIdtoTitle,indexPath,totalDocs,secondaryIndex,searchResult
  loc=bisect(secondaryIndex,word)
  postingList=[]
  # print("loc:",loc)
  if(loc>0):
    postingList=getList(loc,word)
    # print("splitted:",postingList)
  docsperTerm=len(postingList)
  # print(docsperTerm)
  idf=log10(totalDocs/(1+docsperTerm))
  for docEntry in postingList:
    # print("found ",word)
    docId,posting=docEntry.split("#")
    weighted_TF=0
    tagValues=posting.split('+')
    # print("tagValues: ",tagValues)
    for tag in tagValues:
      weighted_TF+=calculateWeight(tag)
      # print("weighted tf:",weighted_TF)
    
    tfidfScore=float(log10(1+weighted_TF)*float(idf))
    if docId=="d542452":
      # print("Mai kr rha hu ",word)
    if docId in searchResult:
      searchResult[docId]+=tfidfScore
    else:
      searchResult[docId]=tfidfScore
    # if(word=='2019'):
    #   print(docId," ",searchResult[docId])

def processPlainQuery(topK,query):
    global searchResult
    # print("here")
    query=processQuery(query)
    # print(query)
    for word in query:
      # print(word)
      calculateScore(word)
      # print(searchResult)
    results=finalResults(topK)
    return results

#Functions to calculate final results and write to file 
def finalResults(topK):
  global DocIdtoTitle,indexPath,totalDocs,secondaryIndex,searchResult
  topKDocs={}
  if topK==0:
    return topKDocs
  searchResult=dict(sorted(searchResult.items(), key=itemgetter(1),reverse=True))
  # print("Sorted Dict: ",searchResult)
  
  # val=1
  # for key in searchResult:
  #   print(key," ",searchResult[key])
  #   if val==10:
  #     break
  #   val+=1
  currK=1
  for key in searchResult:
    docId=key[1:]
    topKDocs[docId]=DocIdtoTitle[docId]
    if currK>=int(topK):
      break
    currK+=1
  return topKDocs

def writeResults(results,time):
    # print(results)
    k=len(results)
    # print(k)
    print(time)
    resultPath="/content/drive/My Drive/wikiResults/queries_op.txt"
    with open(resultPath, 'a') as file:
      for docId in results:
        toWrite=str(docId)+", "+results[docId].strip()+'\n'
        file.write(toWrite)
      avgTime=float(time/k)
      toWrite=str(time)+", "+str(avgTime)+"\n"
      file.write(toWrite)

#Field Query Functions
def getTagQuery(query):
  tagDict={} 
  query=query[1:] # to avoid space after K
  words=query.split(" ")
  currTag=""
  for w in words:
    if ":" in w:
      split_word=w.split(":")
      currTag=split_word[0]
      tagDict[currTag]=[split_word[1]]
    else:
      tagDict[currTag].append(w)
  
  #process all words in a tag
  for key in tagDict:
    queryString=" ".join(tagDict[key])
    tagDict[key]=processQuery(queryString)
  return tagDict

def calculateWeightFieldQuery(tagValue,queryTag):
  tag=tagValue[0]
  freq=int(tagValue[1:])
  # print("tag: ",tag)
  # pritn("queryTag: ",queryTag)
  # print()
  if tag == queryTag:
    # print("matched")
    return freq*10000
  else:
    return freq


def calculateFieldQueryScore(queryTag,wordList):
  global DocIdtoTitle,indexPath,totalDocs,secondaryIndex,searchResult
  # print("quertag: ",queryTag)
  for word in wordList:
    # print("word :",word)
    loc=bisect(secondaryIndex,word)
    postingList=[]
    if(loc>0):
      postingList=getList(loc,word)
    # print("splitted:",postingList)
    docsperTerm=len(postingList)
    # print("total doces for this term: ",docsperTerm)
    idf=log10(totalDocs/(1+docsperTerm))
    for docEntry in postingList:
      docId,posting=docEntry.split("#")
      weighted_TF=0
      tagValues=posting.split('+')
      # print("tagValues: ",tagValues)
      for tagVal in tagValues:
        weighted_TF+=calculateWeightFieldQuery(tagVal,queryTag)
      
      tfidfScore=float(log10(1+weighted_TF)*float(idf))
      if docId in searchResult:
        searchResult[docId]+=tfidfScore
      else:
        searchResult[docId]=tfidfScore
  # print("resut after one tag :",searchResult)

def processFieldQuery(topK,query):
    global searchResult
    tagDict=getTagQuery(query)
    # print(tagDict)
    for tag in tagDict:
      calculateFieldQueryScore(tag,tagDict[tag])
    results=finalResults(topK)
    return results

#First function to start Search Query
def searchQuery():
  global searchResult
  for query in queries:
    start = timeit.default_timer()
    #get K 
    topK,query=query.split(',',1)
    # print("k",topK)
    # print(query)
    fieldQuery=["t:","c:","b:","i:","l:","r:"]
    if any(tag in query for tag in fieldQuery):
      results=processFieldQuery(topK,query)
    else:
      results=processPlainQuery(topK,query)
    stop = timeit.default_timer()
    time=stop-start
    writeResults(results,time)
    searchResult.clear()

if __name__=="__main__":
  getTitle()
  readQueries()
  searchQuery()