{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/apurvi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import Files \n",
    "import xml.sax\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "import bz2\n",
    "from nltk.stem.porter import *\n",
    "import sys\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global Variables \n",
    "Doc_ID=1 #to maintain total Number of pages in the dump\n",
    "Doc_IDtoTitle={} # a map to maintain name of doc ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleProcessor():\n",
    "    def processData(self,data):\n",
    "#         data=self.camelCase(data)\n",
    "#         print(data)\n",
    "        data=self.remove_Regx(data)\n",
    "        data=self.tokenizer(data)\n",
    "        data=self.case_fold(data)\n",
    "        data=self.removeStopwords(data)\n",
    "        data=self.doStemming(data)\n",
    "        return data\n",
    "        \n",
    "#     def camelCase(self,data):\n",
    "#         return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))',data)\n",
    "    \n",
    "    def case_fold(self,data):\n",
    "        case_Folded= [w.casefold() for w in data]\n",
    "        return case_Folded\n",
    "    \n",
    "    def tokenizer(self,data):\n",
    "        tokenizedata=re.findall(\"\\d+|[\\w]+\",data)\n",
    "        tokenizedata=[key for key in tokenizedata]\n",
    "        return tokenizedata\n",
    "    \n",
    "    def removeStopwords(self,data):\n",
    "        stop_word = set(stopwords.words('english'))\n",
    "        removedStopW=[w for w in data if w not in stop_word] \n",
    "        return removedStopW\n",
    "    \n",
    "    def doStemming(self,data):\n",
    "        stemmer=SnowballStemmer(\"english\")\n",
    "        stemmed_data=[]\n",
    "        for words in data:\n",
    "            stemmed_data.append(stemmer.stem(words))\n",
    "        return stemmed_data\n",
    "    \n",
    "    def remove_Regx(self,data):\n",
    "        removReg2=re.compile(r'{\\|(.*?)\\|}',re.DOTALL).sub('',data)\n",
    "        data=removReg2\n",
    "        \n",
    "        removReg3=re.compile(r'{{v?cite(.*?)}}',re.DOTALL).sub('',data)\n",
    "        data=removReg3\n",
    "        \n",
    "        removReg4=re.compile(r'[.,;_()\"/\\']',re.DOTALL).sub(' ',data)\n",
    "        data=removReg4\n",
    "        \n",
    "        removReg5=re.compile(r'\\[\\[file:(.*?)\\]\\]',re.DOTALL).sub('',data)\n",
    "        data=removReg5\n",
    "        \n",
    "        removReg6=re.compile(r'<(.*?)>',re.DOTALL).sub('',data)\n",
    "        data=removReg6\n",
    "        data=data.replace('_',' ').replace(',','')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFieldProcessor():\n",
    "    data=None\n",
    "    def process(self,data):\n",
    "        self.data=data\n",
    "        self.data=self.data.casefold()\n",
    "        self.remove_Regx()\n",
    "        External_Links=self.get_external_links()\n",
    "        info,bodytext,category=self.get_InfoBox_category()\n",
    "        references=self.get_references()\n",
    "#         print(\"info\",info)\n",
    "#         print(\"bodytext\",bodytext)\n",
    "#         print(\"category\",category)\n",
    "#         print(\"external_links\",External_Links)\n",
    "        print(\"reference\",references)\n",
    "        return self.data\n",
    "    \n",
    "    def remove_Regx(self):\n",
    "        removReg1=re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',re.DOTALL).sub('',self.data)\n",
    "        self.data=removReg1\n",
    "        \n",
    "        removReg2=re.compile(r'{\\|(.*?)\\|}',re.DOTALL).sub('',self.data)\n",
    "        self.data=removReg2\n",
    "        \n",
    "        removReg3=re.compile(r'{{v?cite(.*?)}}',re.DOTALL).sub('',self.data)\n",
    "        self.data=removReg3\n",
    "        \n",
    "        removReg4=re.compile(r'[.,;_()\"/\\']',re.DOTALL).sub(' ',self.data)\n",
    "        self.data=removReg4\n",
    "        \n",
    "        removReg5=re.compile(r'\\[\\[file:(.*?)\\]\\]',re.DOTALL).sub('',self.data)\n",
    "        self.data=removReg5\n",
    "        \n",
    "        removReg6=re.compile(r'<(.*?)>',re.DOTALL).sub('',self.data)\n",
    "        self.data=removReg6\n",
    "        self.data=self.data.replace('_',' ').replace(',','')\n",
    "        \n",
    "    def get_Refreg(self):\n",
    "        Ref_regx=re.compile(r'(==\\s?references\\s?==(.*?)}}\\n\\n)', re.DOTALL)\n",
    "        return Ref_regx\n",
    "        \n",
    "    def get_external_links(self):\n",
    "        external_links=[]\n",
    "        lines=self.data.split('\\n')\n",
    "        line_present=1\n",
    "        i=0\n",
    "        while i<len(lines):\n",
    "            if \"== external links ==\" in lines[i] or \"==external links ==\" in lines[i] or \"== external links==\" in lines[i] or line_present==1 or \"==external links==\" in lines[i]:\n",
    "                i+=1\n",
    "                while i < len(lines):\n",
    "                    if \"*[\" in lines[i] or \"* [\" in lines[i]:\n",
    "                        external_links.extend(lines[i].split(' '))\n",
    "                    i+=1\n",
    "            i+=1         \n",
    "        return external_links\n",
    "                \n",
    "    def check_bodyend(self,line):\n",
    "        return \"[[category:\" in line or \"== external links ==\" in line\n",
    "    \n",
    "    def get_InfoBox_category(self):\n",
    "        info=[]\n",
    "        Text_body=[]\n",
    "        Category=[]\n",
    "        flag=0\n",
    "        lines=self.data.split('\\n')\n",
    "        no_line=len(lines)\n",
    "        i=0\n",
    "        Body_flag=1\n",
    "        while(i<no_line):\n",
    "            if '{{infobox' in lines[i]:\n",
    "                count_open=0\n",
    "                split_1=lines[i].split('{{infobox')\n",
    "                if(len(split_1)==2):\n",
    "                    info.extend(split_1[1:])\n",
    "                while True:\n",
    "                    if '{{' in lines[i]:\n",
    "                        openb=lines[i].count('{{')\n",
    "                        flag=1\n",
    "                        count_open+=openb\n",
    "                    \n",
    "                    if '}}' in lines[i]:\n",
    "                        close=lines[i].count('}}')\n",
    "                        flag=0\n",
    "                        count_open-=close\n",
    "                    \n",
    "                    if count_open<=0:\n",
    "                        break\n",
    "                    i=i+1\n",
    "                    \n",
    "                    if(i<len(lines)) and count_open>0:\n",
    "                        append_data=lines[i]\n",
    "                        info.append(append_data)\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            elif (Body_flag==1):\n",
    "                #at start of a category or a external link body finishes\n",
    "                if (self.check_bodyend(lines[i])):\n",
    "                    Body_flag=0\n",
    "                Text_body.append(lines[i])\n",
    "            \n",
    "            elif \"[[category:\" in lines[i]:\n",
    "                category_split=lines[i].split(\"[[category:\")\n",
    "                l=len(category_split)\n",
    "                if(l>1):\n",
    "                    Category.extend(category_split[1:-1])\n",
    "                    l=len(category_split)\n",
    "                    Category.append(category_split[-1].split(']]')[0])\n",
    "            i+=1\n",
    "        \n",
    "        return info,Text_body,Category\n",
    "    \n",
    "    def process_ref(self,Sref):\n",
    "        self.data=self.data.replace(Sref.group(0), ' ')\n",
    "        self.data=self.data.replace(Sref.group(2), ' ')\n",
    "        return Sref.group(2)\n",
    "\n",
    "    def get_references(self):\n",
    "        references=[]\n",
    "        lines=self.data.split('\\n')\n",
    "        line_present=1\n",
    "        i=0\n",
    "        while len(lines)>0 and i<len(lines):\n",
    "            if \"==references==\" in lines[i] or \"== references==\" in lines[i] or \"==references ==\" in lines[i] or \"== references ==\" in lines[i]:\n",
    "                i+=1\n",
    "                flag=1\n",
    "                count_open=0\n",
    "                while i<len(lines):\n",
    "                    if '{{' in lines[i]:\n",
    "                        openb=lines[i].count('{{')\n",
    "                        flag=1\n",
    "                        count_open+=openb\n",
    "                    \n",
    "                    if '}}' in lines[i]:\n",
    "                        close=lines[i].count('}}')\n",
    "                        flag=0\n",
    "                        count_open-=close\n",
    "                    if count_open>0:\n",
    "                        if \"{{reflist\" not in lines[i]:\n",
    "                            references.append(lines[i])\n",
    "                    else:\n",
    "                        break\n",
    "                    i+=1\n",
    "            i+=1\n",
    "        return references\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    global Doc_IDtoTitle\n",
    "    for key in Doc_IDtoTitle:\n",
    "        print(key,\" \",Doc_IDtoTitle[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content Handler code for XML parser\n",
    "def storeValue(key,value,Dict):\n",
    "    global Doc_IDtoTitle\n",
    "    Doc_IDtoTitle[key]=value\n",
    "    \n",
    "class parseXML(xml.sax.ContentHandler):\n",
    "    \n",
    "    global Doc_ID,Doc_IDtoTitle \n",
    "    def __init__(self):\n",
    "        xml.sax.ContentHandler.__init__(self)\n",
    "        self.current_tag=\"\"\n",
    "        self.title_content=\"\"\n",
    "        self.text_content=\"\"\n",
    "    \n",
    "    def startElement(self,tag,attributes):\n",
    "        self.current_tag=tag\n",
    "        if tag=='page':\n",
    "            self.title_content=\"\"\n",
    "            self.text_content=\"\"\n",
    "\n",
    "    def characters(self,content):\n",
    "        if self.current_tag==\"title\":\n",
    "            self.title_content+=content\n",
    "        elif self.current_tag==\"text\":\n",
    "            self.text_content+=content\n",
    "            \n",
    "    def endElement(self,tag):\n",
    "        if tag=='page':\n",
    "            global Doc_ID\n",
    "            print(\"----------------------end doc\",Doc_ID,\"----------------------------\")\n",
    "            if Doc_ID==20:\n",
    "#                 check()\n",
    "                sys.exit()\n",
    "            Doc_ID+=1\n",
    "        \n",
    "        elif tag=='title':\n",
    "#             print(self.title_content)\n",
    "            storeValue(Doc_ID,self.title_content,Doc_IDtoTitle) \n",
    "        \n",
    "        elif tag=='text':\n",
    "            #process title\n",
    "            pr=TitleProcessor()\n",
    "            processedTitle=pr.processData(self.title_content)\n",
    "#             print(\"Processed title : \",processedTitle)\n",
    "            \n",
    "            #get fields from text_content\n",
    "            pt=TextFieldProcessor()\n",
    "            pt.process(self.text_content)\n",
    "        self.current_tag=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference []\n",
      "----------------------end doc 1 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 2 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 3 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 4 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 5 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 6 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 7 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 8 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 9 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 10 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 11 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 12 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 13 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 14 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 15 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 16 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 17 ----------------------------\n",
      "reference ['', '', '', '', '', '', '', '', '', '', '', '', ' -->', '', '[  project amazonia: characterization – abiotic – water -->', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "----------------------end doc 18 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 19 ----------------------------\n",
      "reference []\n",
      "----------------------end doc 20 ----------------------------\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    Wiki_input_file=\"./Data/input.bz2\"\n",
    "    source = bz2.BZ2File(Wiki_input_file, \"rb\")\n",
    "\n",
    "    #Create a parser Object\n",
    "    parser= xml.sax.make_parser()\n",
    "\n",
    "    #turnoff namespaces\n",
    "    parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "    \n",
    "    #callHandler\n",
    "    handler= parseXML()\n",
    "    \n",
    "    #parse data \n",
    "    parser.setContentHandler(handler)\n",
    "    parser.parse(source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
