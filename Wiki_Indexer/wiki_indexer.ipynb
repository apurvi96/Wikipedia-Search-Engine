{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Files \n",
    "import xml.sax\n",
    "import bz2\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from titleProcessor import TitleProcessor\n",
    "from TextParser import TextFieldProcessor\n",
    "from PageProcessor import PageProcessor\n",
    "import re\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global Variables \n",
    "Doc_ID=1 #to maintain total Number of pages in the dump\n",
    "Doc_IDtoTitle={} # a map to maintain name of doc ids\n",
    "globalDictionary={} #final invertedIndex dictionary for each batch\n",
    "invertedIndexFile=1 #maintainsFile number\n",
    "totalDumpTokens=0\n",
    "invertedIndexTokens=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGlobalDictionary(pageDictionary):\n",
    "    global globalDictionary,Doc_ID\n",
    "    for word in pageDictionary:\n",
    "        check=checkWords(word)\n",
    "        if check==True:\n",
    "            continue\n",
    "        pageDict={}\n",
    "        if word in globalDictionary :\n",
    "            pageDict=globalDictionary[word]\n",
    "        pageDict[Doc_ID]=pageDictionary[word]\n",
    "        globalDictionary[word]=pageDict\n",
    "\n",
    "def writeToFile():\n",
    "    global globalDictionary,invertedIndexFile,Doc_ID\n",
    "#     filePath=\"indexfile\"+str(invertedIndexFile)+\".txt\"\n",
    "    folderPath=sys.argv[2]\n",
    "    filePath=folderPath+\"indexfile\"+str(invertedIndexFile)+\".txt\"\n",
    "    invertedIndexFile+=1\n",
    "    with open(filePath, \"a\") as outfile:\n",
    "        for word in sorted(globalDictionary):\n",
    "            invStr=word+\":\"\n",
    "#             print(\"in write file \",invStr)\n",
    "            tempDict=globalDictionary[word]\n",
    "            for item in sorted(tempDict):\n",
    "                invStr+=\"d\"+str(item)\n",
    "    #             print(\"add docid\",invStr)\n",
    "                fieldFreq=tempDict[item]\n",
    "                tagDict={0:\"t\",1:\"i\",2:\"b\",3:\"c\",4:\"l\",5:\"r\"}\n",
    "                for i in range(0,len(fieldFreq)):\n",
    "    #                 print(\"i is\",i,fieldFreq[i])\n",
    "                    if fieldFreq[i]>0:\n",
    "                        invStr+=\"#\"+tagDict[i]+str(fieldFreq[i])\n",
    "                invStr+=\"|\"        \n",
    "            invStr=invStr[:-1]\n",
    "#             print(\"final invStr\",invStr)\n",
    "#             print(invStr)\n",
    "            outfile.write(invStr + '\\n')        \n",
    "    outfile.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkWords(word):\n",
    "    ignoreWords=['redirect','redirect2']\n",
    "    if len(word)<=2 or word in ignoreWords or re.match('^[0]+$',word) or not(re.match('^[a-zA-Z0-9]+$',word)) or (word.isdecimal() and len(word)>4):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStatFile():\n",
    "    global globalDictionary,totalDumpTokens\n",
    "    filePath=sys.argv[3]\n",
    "#     filePath=\"stat.txt\"\n",
    "    with open(filePath, \"w\") as outfile:\n",
    "#         print(\"final totalDumpTokens \",totalDumpTokens)\n",
    "        outfile.write(str(totalDumpTokens)+ '\\n')\n",
    "#         print(\"final global \",len(globalDictionary))\n",
    "        outfile.write(str(len(globalDictionary)))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content Handler code for XML parser\n",
    "def storeValue(key,value,Dict):\n",
    "    global Doc_IDtoTitle\n",
    "    Doc_IDtoTitle[key]=value\n",
    "    \n",
    "class parseXML(xml.sax.ContentHandler):\n",
    "    \n",
    "    global Doc_ID,Doc_IDtoTitle \n",
    "    def __init__(self):\n",
    "        xml.sax.ContentHandler.__init__(self)\n",
    "        self.current_tag=\"\"\n",
    "        self.title_content=\"\"\n",
    "        self.text_content=\"\"\n",
    "    \n",
    "    def startElement(self,tag,attributes):\n",
    "        self.current_tag=tag\n",
    "        if tag=='page':\n",
    "            self.title_content=\"\"\n",
    "            self.text_content=\"\"\n",
    "\n",
    "    def characters(self,content):\n",
    "        if self.current_tag==\"title\":\n",
    "            self.title_content+=content\n",
    "        elif self.current_tag==\"text\":\n",
    "            self.text_content+=content\n",
    "            \n",
    "    def endElement(self,tag):\n",
    "        if tag=='page':\n",
    "            global Doc_ID,globalDictionary,Doc_IDtoTitle\n",
    "#             print(\"----------------------end doc\",Doc_ID,\"----------------------------\")\n",
    "            if Doc_ID%50000==0:\n",
    "#             if Doc_ID==10:\n",
    "                writeToFile()\n",
    "                globalDictionary.clear()\n",
    "                Doc_IDtoTitle.clear()\n",
    "#                 sys.exit()\n",
    "           \n",
    "            Doc_ID+=1\n",
    "        \n",
    "        elif tag=='title':\n",
    "#             print(self.title_content)\n",
    "#             global Doc_ID,globalDictionary,Doc_IDtoTitle\n",
    "            storeValue(Doc_ID,self.title_content,Doc_IDtoTitle) \n",
    "        \n",
    "        elif tag=='text':\n",
    "            global totalDumpTokens\n",
    "            #process title\n",
    "            pr=TitleProcessor()\n",
    "            processedTitle,prevTitle=pr.processData(self.title_content)\n",
    "            \n",
    "            #get fields from text_content\n",
    "            pt=TextFieldProcessor()\n",
    "            info,bodyText,category,externalLinks,references=pt.process(self.text_content)\n",
    "            \n",
    "            #make dictionary for each field\n",
    "            titleDict=pr.getTitleDictionary(processedTitle)\n",
    "            page=PageProcessor()\n",
    "            infoDict,prevInfo=page.getAllDictionary(info)\n",
    "#             print(\"---------info---------\")\n",
    "            bodyDict,prevBody=page.getAllDictionary(bodyText)\n",
    "#             print(\"---------bodyDict---------\")\n",
    "            categoryDict,prevCat=page.getAllDictionary(category)\n",
    "#             print(\"---------catDict---------\")\n",
    "            externalLinksDict,prevLinks=page.getAllDictionary(externalLinks)\n",
    "#             print(\"---------exlDict---------\")\n",
    "#             print(referencesDict)\n",
    "            referencesDict,prevRef=page.getAllDictionary(references)\n",
    "            \n",
    "            totalDumpTokens+=(prevTitle+prevLinks+prevCat+prevBody+prevInfo+prevTitle)\n",
    "            #make final page Dictionary for each page\n",
    "            pageDictionary=page.getPageDictionary(titleDict,infoDict,bodyDict,categoryDict,externalLinksDict,referencesDict)\n",
    "#             print(\"page dictionary \",pageDictionary)\n",
    "            getGlobalDictionary(pageDictionary)\n",
    "        self.current_tag=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "page Dict 7\n",
      "global  0\n",
      "----------------------end doc 1 ----------------------------\n",
      "11862\n",
      "page Dict 2090\n",
      "global  6\n",
      "----------------------end doc 2 ----------------------------\n",
      "final totalDumpTokens  11862\n",
      "final global  2001\n",
      "Time:  0.6826981189951766\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    source=sys.argv[1]\n",
    "#     Wiki_input_file=\"./Data/input.bz2\"\n",
    "#     source = bz2.BZ2File(Wiki_input_file, \"rb\")\n",
    "#     source=\"/home/apurvi/Desktop/SEM-3/IRE/Mini Project/Wikipedia Search Engine/multistream2.xml-p1p30303\"\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    #Create a parser Object\n",
    "    parser= xml.sax.make_parser()\n",
    "    \n",
    "    #turnoff namespaces\n",
    "    parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "    \n",
    "    #callHandler\n",
    "    handler= parseXML()\n",
    "    \n",
    "    #parse data \n",
    "    parser.setContentHandler(handler)\n",
    "    parser.parse(source)\n",
    "    \n",
    "    if bool(globalDictionary):\n",
    "        writeToFile()\n",
    "    writeStatFile()\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    print('Time: ', stop - start)\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
